{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIDPhiigMKQI"
      },
      "source": [
        "# User Defined Functions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgS3nhjaMQQt"
      },
      "source": [
        "## Prepare environment\n",
        "First, we are going to prepare the environment for running PySaprk in the Google Collab Machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTEPUoaL69gw",
        "outputId": "c28106ad-e863-468f-ed90-4888ce2ae762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsdkaEd97EHI",
        "outputId": "e85d3f2b-6478-459a-da49-1931a59a2fb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Install JAVA 8\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (3.2)\n",
            "Obtaining last version of spark\n",
            "/content/drive/MyDrive/colab/massive/install_pyspark.py:17: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 17 of the file /content/drive/MyDrive/colab/massive/install_pyspark.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  soup = BeautifulSoup(html_doc)\n",
            "Getting version spark-3.5.1\n",
            "Downloading https://downloads.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
            "Installing PySpark\n",
            "Setting environment variables for JAVA_HOME and SPARK_HOME\n"
          ]
        }
      ],
      "source": [
        "!python /content/drive/MyDrive/colab/massive/install_pyspark.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3_oicKBMsEO"
      },
      "source": [
        "## Start working with Spark\n",
        "Now we now and understand how Spark appeared in our lives and more or less how it works (and you know, it's amazing 游뱘), we can start to work with it.\n",
        "As you now, the SparkSession is the way programmers \"talk\" with Spark. So, we need to inicialize that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "ekZc-YctYpwJ"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (SparkSession\n",
        " .builder\n",
        " .appName(\"example\")\n",
        " .getOrCreate())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQHcAB5cOBxV"
      },
      "source": [
        "## Create a DF to program the example mentioned in slides"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYZQkqf37lm4",
        "outputId": "dc2946e7-1aad-4410-ebf4-c62cedebb2e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+---+\n",
            "|             name|age|\n",
            "+-----------------+---+\n",
            "|    juan fernando| 20|\n",
            "|valentina laverde| 31|\n",
            "|   teresa s치nchez| 30|\n",
            "|    julieta ponce| 35|\n",
            "|   antonio garc칤a| 25|\n",
            "+-----------------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.createDataFrame([(\"juan fernando\", 20), (\"valentina laverde\", 31), (\"teresa s치nchez\", 30), (\"julieta ponce\", 35), (\"antonio garc칤a\", 25)], [\"name\", \"age\"])\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fU_y-oj-c3HS"
      },
      "source": [
        "Remember, what we want is to convert the first letter to capital case.\n",
        "Fist, we need to create a python function, that from a given input (string) it converts the value into same string with first letter as capital case letter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "IoDgTh6Pc2ON"
      },
      "outputs": [],
      "source": [
        "def convertCase(lower_string):\n",
        "    result=\"\"\n",
        "    arr = lower_string.split(\" \")\n",
        "    for x in arr:\n",
        "       result= result + x[0].upper() + x[1:len(x)] + \" \"\n",
        "    return result[0:-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyPo6_kZeAvF"
      },
      "source": [
        "Now, we convert the funciton to udf (the default type of UDF is StringType)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "UXKli4qed7Z4"
      },
      "outputs": [],
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "convertUDF = F.udf(lambda z: convertCase(z))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjmmQwzIedgs"
      },
      "source": [
        "Now, we can use the convertUDF, as a function of sparkSQL, for example, in a select() or in a withColumn() call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-HzXmlfeZUe",
        "outputId": "e1434efb-f995-4a98-c091-f9d5ecd2ca3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+---+\n",
            "|name             |age|\n",
            "+-----------------+---+\n",
            "|Juan Fernando    |20 |\n",
            "|Valentina Laverde|31 |\n",
            "|Teresa S치nchez   |30 |\n",
            "|Julieta Ponce    |35 |\n",
            "|Antonio Garc칤a   |25 |\n",
            "+-----------------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select(convertUDF(F.col(\"name\")).alias(\"name\"), F.col(\"age\") ) \\\n",
        "   .show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMu8m3zme5-9",
        "outputId": "0c18ce51-4bc9-4c8a-d7d6-988d3f60a826"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+---+-----------------+\n",
            "|name             |age|corrected name   |\n",
            "+-----------------+---+-----------------+\n",
            "|juan fernando    |20 |Juan Fernando    |\n",
            "|valentina laverde|31 |Valentina Laverde|\n",
            "|teresa s치nchez   |30 |Teresa S치nchez   |\n",
            "|julieta ponce    |35 |Julieta Ponce    |\n",
            "|antonio garc칤a   |25 |Antonio Garc칤a   |\n",
            "+-----------------+---+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.withColumn(\"corrected name\", convertUDF(F.col(\"name\")))\\\n",
        "  .show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0beyu5DqfmIA"
      },
      "source": [
        "We can also use our UDF on SQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piR_nNhxfH3U",
        "outputId": "c759c831-9bae-481b-d9d5-52fc9cd08043"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+---+\n",
            "|name             |age|\n",
            "+-----------------+---+\n",
            "|Juan Fernando    |20 |\n",
            "|Valentina Laverde|31 |\n",
            "|Teresa S치nchez   |30 |\n",
            "|Julieta Ponce    |35 |\n",
            "|Antonio Garc칤a   |25 |\n",
            "+-----------------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pyspark.sql.types as T\n",
        "spark.udf.register(\"convertUDF\", convertCase, T.StringType())\n",
        "df.createOrReplaceTempView(\"NAMES\")\n",
        "spark.sql(\"select convertUDF(name) as name, age from NAMES\") \\\n",
        "     .show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkD0YH-vg5nv"
      },
      "source": [
        "Another way to create UDF method, is to use the annotation @udf(resturnType=\\<type\\>) above the method definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "gjRGgRBSgHGs"
      },
      "outputs": [],
      "source": [
        "@F.udf(returnType=T.StringType())\n",
        "def upperCase(str):\n",
        "    return str.upper()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LN_z9kkhQd8",
        "outputId": "d2049045-5da4-4649-e015-5f64288639dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+---+-----------------+\n",
            "|name             |age|Upper Name       |\n",
            "+-----------------+---+-----------------+\n",
            "|juan fernando    |20 |JUAN FERNANDO    |\n",
            "|valentina laverde|31 |VALENTINA LAVERDE|\n",
            "|teresa s치nchez   |30 |TERESA S츼NCHEZ   |\n",
            "|julieta ponce    |35 |JULIETA PONCE    |\n",
            "|antonio garc칤a   |25 |ANTONIO GARC칈A   |\n",
            "+-----------------+---+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.withColumn(\"Upper Name\", upperCase(F.col(\"Name\"))) \\\n",
        ".show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WHG3XNazyEN"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WLB9TyljwAt"
      },
      "source": [
        "## Handling null check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwQn_-6gj0Fg",
        "outputId": "b6cf2706-8eac-4d0a-c507-196348aa0734"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+---+\n",
            "|          name|age|\n",
            "+--------------+---+\n",
            "| juan fernando| 20|\n",
            "|          NULL| 31|\n",
            "|teresa s치nchez| 30|\n",
            "| julieta ponce| 35|\n",
            "|antonio garc칤a| 25|\n",
            "+--------------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_nulls = spark.createDataFrame([(\"juan fernando\", 20), (None, 31), (\"teresa s치nchez\", 30), (\"julieta ponce\", 35), (\"antonio garc칤a\", 25)], [\"name\", \"age\"])\n",
        "df_nulls.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHzNcCUyhXg2",
        "outputId": "b69d1c27-3d06-40c1-b03a-2034d775f48c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "|Name         |\n",
            "+-------------+\n",
            "|Juan Fernando|\n",
            "+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_nulls.createOrReplaceTempView(\"NAMES_NULLS\")\n",
        "spark.sql(\"select convertUDF(name) as Name from NAMES_NULLS \" + \\\n",
        "         \"where name is not null and convertUDF(name) like '%Juan%'\") \\\n",
        "     .show(truncate=False)\n",
        "#IT COULD FAIL if the udf is executed befoure the not null check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngLDG9sj1KGI"
      },
      "source": [
        "To aboid this, we can filter nulls in the registration of the UDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zO-Js0wifnW",
        "outputId": "ceb8cf04-71bc-43c9-9375-b32b3275f295"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.<lambda>(str)>"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "spark.udf.register(\"_nullsafeUDF\", lambda str: convertCase(str) if not str is None else \"\" , T.StringType())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvVe8Adnjjmq",
        "outputId": "289bc44b-87fe-4de2-bf54-81e71e47b269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "|Name         |\n",
            "+-------------+\n",
            "|Juan Fernando|\n",
            "+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"select _nullsafeUDF(name) as Name from NAMES_NULLS \" + \\\n",
        "         \"where _nullsafeUDF(name) like '%Juan%'\") \\\n",
        "     .show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "046hSw2A28NG"
      },
      "source": [
        "# Exercise 1:\n",
        "\n",
        "\n",
        "\n",
        "*   Get data from the CSV: https://raw.githubusercontent.com/tidyverse/ggplot2/main/data-raw/diamonds.csv and save it in a dataframe.\n",
        "*   Generate a new column, called \"cut_color_id\". This column, will have the first letter of the *cut* column, and the *color* column value. As an example, if the *cut* is \"Premium\" and the *color* is \"I\", the result in the new column will be \"PI\". Do it with a UDF.\n",
        "*   Take into account, is better to use the functions of spark, if we can, because they are more optized than UDFs. Do you know how to do the same without an UDF? Do it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import urllib.request\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"diamonds\").getOrCreate()\n",
        "\n",
        "# URL of the CSV file\n",
        "url = \"https://raw.githubusercontent.com/tidyverse/ggplot2/main/data-raw/diamonds.csv\"\n",
        "\n",
        "# Download the file locally\n",
        "local_path = \"/content/drive/MyDrive/colab/massive/diamonds.csv\"\n",
        "urllib.request.urlretrieve(url, local_path)\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "diamonds_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(local_path)\n",
        "diamonds_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VkQRgI_g1uL",
        "outputId": "140f4e65-f035-412e-9e23-fc39c90fb2ad"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------+-----+-------+-----+-----+-----+----+----+----+\n",
            "|carat|      cut|color|clarity|depth|table|price|   x|   y|   z|\n",
            "+-----+---------+-----+-------+-----+-----+-----+----+----+----+\n",
            "| 0.23|    Ideal|    E|    SI2| 61.5| 55.0|  326|3.95|3.98|2.43|\n",
            "| 0.21|  Premium|    E|    SI1| 59.8| 61.0|  326|3.89|3.84|2.31|\n",
            "| 0.23|     Good|    E|    VS1| 56.9| 65.0|  327|4.05|4.07|2.31|\n",
            "| 0.29|  Premium|    I|    VS2| 62.4| 58.0|  334| 4.2|4.23|2.63|\n",
            "| 0.31|     Good|    J|    SI2| 63.3| 58.0|  335|4.34|4.35|2.75|\n",
            "| 0.24|Very Good|    J|   VVS2| 62.8| 57.0|  336|3.94|3.96|2.48|\n",
            "| 0.24|Very Good|    I|   VVS1| 62.3| 57.0|  336|3.95|3.98|2.47|\n",
            "| 0.26|Very Good|    H|    SI1| 61.9| 55.0|  337|4.07|4.11|2.53|\n",
            "| 0.22|     Fair|    E|    VS2| 65.1| 61.0|  337|3.87|3.78|2.49|\n",
            "| 0.23|Very Good|    H|    VS1| 59.4| 61.0|  338| 4.0|4.05|2.39|\n",
            "|  0.3|     Good|    J|    SI1| 64.0| 55.0|  339|4.25|4.28|2.73|\n",
            "| 0.23|    Ideal|    J|    VS1| 62.8| 56.0|  340|3.93| 3.9|2.46|\n",
            "| 0.22|  Premium|    F|    SI1| 60.4| 61.0|  342|3.88|3.84|2.33|\n",
            "| 0.31|    Ideal|    J|    SI2| 62.2| 54.0|  344|4.35|4.37|2.71|\n",
            "|  0.2|  Premium|    E|    SI2| 60.2| 62.0|  345|3.79|3.75|2.27|\n",
            "| 0.32|  Premium|    E|     I1| 60.9| 58.0|  345|4.38|4.42|2.68|\n",
            "|  0.3|    Ideal|    I|    SI2| 62.0| 54.0|  348|4.31|4.34|2.68|\n",
            "|  0.3|     Good|    J|    SI1| 63.4| 54.0|  351|4.23|4.29| 2.7|\n",
            "|  0.3|     Good|    J|    SI1| 63.8| 56.0|  351|4.23|4.26|2.71|\n",
            "|  0.3|Very Good|    J|    SI1| 62.7| 59.0|  351|4.21|4.27|2.66|\n",
            "+-----+---------+-----+-------+-----+-----+-----+----+----+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# Define the UDF that takes two columns and concatenates the first letter of 'cut' with 'color'\n",
        "def concat_cut_color(cut, color):\n",
        "    return cut[0] + color\n",
        "\n",
        "# Register the UDF\n",
        "concat_cut_color_udf = udf(concat_cut_color, StringType())\n",
        "\n",
        "# Apply the UDF to create a new column 'cut_color_id'\n",
        "diamonds_df_new = diamonds_df.withColumn(\"cut_color_id\", concat_cut_color_udf(diamonds_df[\"cut\"], diamonds_df[\"color\"]))\n",
        "\n",
        "# Show the updated DataFrame\n",
        "diamonds_df_new.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOpwNcV3g08s",
        "outputId": "5d6f63f9-d618-4f49-a593-e6536f42763f"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------+-----+-------+-----+-----+-----+----+----+----+------------+\n",
            "|carat|      cut|color|clarity|depth|table|price|   x|   y|   z|cut_color_id|\n",
            "+-----+---------+-----+-------+-----+-----+-----+----+----+----+------------+\n",
            "| 0.23|    Ideal|    E|    SI2| 61.5| 55.0|  326|3.95|3.98|2.43|          IE|\n",
            "| 0.21|  Premium|    E|    SI1| 59.8| 61.0|  326|3.89|3.84|2.31|          PE|\n",
            "| 0.23|     Good|    E|    VS1| 56.9| 65.0|  327|4.05|4.07|2.31|          GE|\n",
            "| 0.29|  Premium|    I|    VS2| 62.4| 58.0|  334| 4.2|4.23|2.63|          PI|\n",
            "| 0.31|     Good|    J|    SI2| 63.3| 58.0|  335|4.34|4.35|2.75|          GJ|\n",
            "| 0.24|Very Good|    J|   VVS2| 62.8| 57.0|  336|3.94|3.96|2.48|          VJ|\n",
            "| 0.24|Very Good|    I|   VVS1| 62.3| 57.0|  336|3.95|3.98|2.47|          VI|\n",
            "| 0.26|Very Good|    H|    SI1| 61.9| 55.0|  337|4.07|4.11|2.53|          VH|\n",
            "| 0.22|     Fair|    E|    VS2| 65.1| 61.0|  337|3.87|3.78|2.49|          FE|\n",
            "| 0.23|Very Good|    H|    VS1| 59.4| 61.0|  338| 4.0|4.05|2.39|          VH|\n",
            "|  0.3|     Good|    J|    SI1| 64.0| 55.0|  339|4.25|4.28|2.73|          GJ|\n",
            "| 0.23|    Ideal|    J|    VS1| 62.8| 56.0|  340|3.93| 3.9|2.46|          IJ|\n",
            "| 0.22|  Premium|    F|    SI1| 60.4| 61.0|  342|3.88|3.84|2.33|          PF|\n",
            "| 0.31|    Ideal|    J|    SI2| 62.2| 54.0|  344|4.35|4.37|2.71|          IJ|\n",
            "|  0.2|  Premium|    E|    SI2| 60.2| 62.0|  345|3.79|3.75|2.27|          PE|\n",
            "| 0.32|  Premium|    E|     I1| 60.9| 58.0|  345|4.38|4.42|2.68|          PE|\n",
            "|  0.3|    Ideal|    I|    SI2| 62.0| 54.0|  348|4.31|4.34|2.68|          II|\n",
            "|  0.3|     Good|    J|    SI1| 63.4| 54.0|  351|4.23|4.29| 2.7|          GJ|\n",
            "|  0.3|     Good|    J|    SI1| 63.8| 56.0|  351|4.23|4.26|2.71|          GJ|\n",
            "|  0.3|Very Good|    J|    SI1| 62.7| 59.0|  351|4.21|4.27|2.66|          VJ|\n",
            "+-----+---------+-----+-------+-----+-----+-----+----+----+----+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import concat, col, substring\n",
        "\n",
        "# Assuming diamonds_df is your DataFrame\n",
        "# Create a new column 'cut_color_id' by concatenating the first letter of 'cut' with 'color'\n",
        "diamonds_df.show()\n",
        "diamonds_df = diamonds_df.withColumn(\"cut_color_id\", concat(substring(col(\"cut\"), 1, 1), col(\"color\")))\n",
        "\n",
        "# Show the updated DataFrame to verify the new column\n",
        "diamonds_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8EhsAr-n6N9",
        "outputId": "0c34d95e-9c4d-442e-a1ad-6f9dafde9781"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------+-----+-------+-----+-----+-----+----+----+----+\n",
            "|carat|      cut|color|clarity|depth|table|price|   x|   y|   z|\n",
            "+-----+---------+-----+-------+-----+-----+-----+----+----+----+\n",
            "| 0.23|    Ideal|    E|    SI2| 61.5| 55.0|  326|3.95|3.98|2.43|\n",
            "| 0.21|  Premium|    E|    SI1| 59.8| 61.0|  326|3.89|3.84|2.31|\n",
            "| 0.23|     Good|    E|    VS1| 56.9| 65.0|  327|4.05|4.07|2.31|\n",
            "| 0.29|  Premium|    I|    VS2| 62.4| 58.0|  334| 4.2|4.23|2.63|\n",
            "| 0.31|     Good|    J|    SI2| 63.3| 58.0|  335|4.34|4.35|2.75|\n",
            "| 0.24|Very Good|    J|   VVS2| 62.8| 57.0|  336|3.94|3.96|2.48|\n",
            "| 0.24|Very Good|    I|   VVS1| 62.3| 57.0|  336|3.95|3.98|2.47|\n",
            "| 0.26|Very Good|    H|    SI1| 61.9| 55.0|  337|4.07|4.11|2.53|\n",
            "| 0.22|     Fair|    E|    VS2| 65.1| 61.0|  337|3.87|3.78|2.49|\n",
            "| 0.23|Very Good|    H|    VS1| 59.4| 61.0|  338| 4.0|4.05|2.39|\n",
            "|  0.3|     Good|    J|    SI1| 64.0| 55.0|  339|4.25|4.28|2.73|\n",
            "| 0.23|    Ideal|    J|    VS1| 62.8| 56.0|  340|3.93| 3.9|2.46|\n",
            "| 0.22|  Premium|    F|    SI1| 60.4| 61.0|  342|3.88|3.84|2.33|\n",
            "| 0.31|    Ideal|    J|    SI2| 62.2| 54.0|  344|4.35|4.37|2.71|\n",
            "|  0.2|  Premium|    E|    SI2| 60.2| 62.0|  345|3.79|3.75|2.27|\n",
            "| 0.32|  Premium|    E|     I1| 60.9| 58.0|  345|4.38|4.42|2.68|\n",
            "|  0.3|    Ideal|    I|    SI2| 62.0| 54.0|  348|4.31|4.34|2.68|\n",
            "|  0.3|     Good|    J|    SI1| 63.4| 54.0|  351|4.23|4.29| 2.7|\n",
            "|  0.3|     Good|    J|    SI1| 63.8| 56.0|  351|4.23|4.26|2.71|\n",
            "|  0.3|Very Good|    J|    SI1| 62.7| 59.0|  351|4.21|4.27|2.66|\n",
            "+-----+---------+-----+-------+-----+-----+-----+----+----+----+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----+---------+-----+-------+-----+-----+-----+----+----+----+------------+\n",
            "|carat|      cut|color|clarity|depth|table|price|   x|   y|   z|cut_color_id|\n",
            "+-----+---------+-----+-------+-----+-----+-----+----+----+----+------------+\n",
            "| 0.23|    Ideal|    E|    SI2| 61.5| 55.0|  326|3.95|3.98|2.43|          IE|\n",
            "| 0.21|  Premium|    E|    SI1| 59.8| 61.0|  326|3.89|3.84|2.31|          PE|\n",
            "| 0.23|     Good|    E|    VS1| 56.9| 65.0|  327|4.05|4.07|2.31|          GE|\n",
            "| 0.29|  Premium|    I|    VS2| 62.4| 58.0|  334| 4.2|4.23|2.63|          PI|\n",
            "| 0.31|     Good|    J|    SI2| 63.3| 58.0|  335|4.34|4.35|2.75|          GJ|\n",
            "| 0.24|Very Good|    J|   VVS2| 62.8| 57.0|  336|3.94|3.96|2.48|          VJ|\n",
            "| 0.24|Very Good|    I|   VVS1| 62.3| 57.0|  336|3.95|3.98|2.47|          VI|\n",
            "| 0.26|Very Good|    H|    SI1| 61.9| 55.0|  337|4.07|4.11|2.53|          VH|\n",
            "| 0.22|     Fair|    E|    VS2| 65.1| 61.0|  337|3.87|3.78|2.49|          FE|\n",
            "| 0.23|Very Good|    H|    VS1| 59.4| 61.0|  338| 4.0|4.05|2.39|          VH|\n",
            "|  0.3|     Good|    J|    SI1| 64.0| 55.0|  339|4.25|4.28|2.73|          GJ|\n",
            "| 0.23|    Ideal|    J|    VS1| 62.8| 56.0|  340|3.93| 3.9|2.46|          IJ|\n",
            "| 0.22|  Premium|    F|    SI1| 60.4| 61.0|  342|3.88|3.84|2.33|          PF|\n",
            "| 0.31|    Ideal|    J|    SI2| 62.2| 54.0|  344|4.35|4.37|2.71|          IJ|\n",
            "|  0.2|  Premium|    E|    SI2| 60.2| 62.0|  345|3.79|3.75|2.27|          PE|\n",
            "| 0.32|  Premium|    E|     I1| 60.9| 58.0|  345|4.38|4.42|2.68|          PE|\n",
            "|  0.3|    Ideal|    I|    SI2| 62.0| 54.0|  348|4.31|4.34|2.68|          II|\n",
            "|  0.3|     Good|    J|    SI1| 63.4| 54.0|  351|4.23|4.29| 2.7|          GJ|\n",
            "|  0.3|     Good|    J|    SI1| 63.8| 56.0|  351|4.23|4.26|2.71|          GJ|\n",
            "|  0.3|Very Good|    J|    SI1| 62.7| 59.0|  351|4.21|4.27|2.66|          VJ|\n",
            "+-----+---------+-----+-------+-----+-----+-----+----+----+----+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5Q2_HiZTJs1"
      },
      "source": [
        "# Caching and Persistence of Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2R7JYa2TOZj"
      },
      "source": [
        "# DataFrame.cache()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXANSZ-ijl6f",
        "outputId": "9b0cf6e9-a083-4c41-ca59-b4fb65195db2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "| id|sqaure|\n",
            "+---+------+\n",
            "|  0|     0|\n",
            "|  1|     1|\n",
            "|  2|     4|\n",
            "|  3|     9|\n",
            "|  4|    16|\n",
            "|  5|    25|\n",
            "|  6|    36|\n",
            "|  7|    49|\n",
            "|  8|    64|\n",
            "|  9|    81|\n",
            "| 10|   100|\n",
            "| 11|   121|\n",
            "| 12|   144|\n",
            "| 13|   169|\n",
            "| 14|   196|\n",
            "| 15|   225|\n",
            "| 16|   256|\n",
            "| 17|   289|\n",
            "| 18|   324|\n",
            "| 19|   361|\n",
            "+---+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_to_cache = spark.range(1*10000000).toDF(\"id\").withColumn(\"sqaure\", F.col(\"id\")*F.col(\"id\"))\n",
        "df_to_cache.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxFpdRGGTUR0",
        "outputId": "253fb4c1-4cd0-4900-85af-7b2d4ffe2c23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, sqaure: bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ],
      "source": [
        "#cache this data\n",
        "df_to_cache.cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uursOPSTs-U",
        "outputId": "e9d60d12-310b-4a50-cbb4-e0d7442eb150"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.15850915600000093"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "startTimeQuery = time.process_time()\n",
        "df_to_cache.count()\n",
        "endTimeQuery = time.process_time()\n",
        "endTimeQuery - startTimeQuery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTJFJCKCTwZE",
        "outputId": "ea0559ff-976f-431d-ab9c-4b167f7d95a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.004738387000003286"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "source": [
        "startTimeQuery = time.process_time()\n",
        "df_to_cache.count()\n",
        "endTimeQuery = time.process_time()\n",
        "endTimeQuery - startTimeQuery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "9fsr6DhvVdfN"
      },
      "outputs": [],
      "source": [
        "df_to_persist = spark.range(10001000).toDF(\"id2\").withColumn(\"sqaure\", F.col(\"id2\")*F.col(\"id2\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF0NT9kQYsVn",
        "outputId": "537c4bdb-2735-40da-c601-d310dcba64b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id2: bigint, sqaure: bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ],
      "source": [
        "from pyspark.storagelevel import StorageLevel\n",
        "#persist this data\n",
        "df_to_persist.persist(StorageLevel.DISK_ONLY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVWjyivuY5bO",
        "outputId": "9e799150-bccb-46ff-9f90-df3412bddd5d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1580495159999984"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "startTimeQuery = time.process_time()\n",
        "df_to_persist.count()\n",
        "endTimeQuery = time.process_time()\n",
        "endTimeQuery - startTimeQuery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNWrI4mVY_N2",
        "outputId": "61b37558-fe58-4cc7-a021-36da16be86e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0056970289999966894"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ],
      "source": [
        "startTimeQuery = time.process_time()\n",
        "df_to_persist.count()\n",
        "endTimeQuery = time.process_time()\n",
        "endTimeQuery - startTimeQuery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKZCvvN0aptG"
      },
      "source": [
        "As this data is now saved on disk, after use it, we are going to erase it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQvN6--oZpde",
        "outputId": "9ca79f67-dec5-4992-e087-313dd59f49f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id2: bigint, sqaure: bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ],
      "source": [
        "df_to_persist.unpersist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E65s2RFdazf-",
        "outputId": "c09b0c39-0dac-4406-f087-93b9ba7f076f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, sqaure: bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ],
      "source": [
        "df_to_cache.unpersist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "3O0toVoM0OQu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}