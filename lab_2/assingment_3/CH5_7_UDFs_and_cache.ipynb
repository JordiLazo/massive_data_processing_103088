{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CH5&7 - UDFs and cache.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# User Defined Functions\n",
        "\n"
      ],
      "metadata": {
        "id": "OIDPhiigMKQI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare environment\n",
        "First, we are going to prepare the environment for running PySaprk in the Google Collab Machine"
      ],
      "metadata": {
        "id": "bgS3nhjaMQQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTEPUoaL69gw",
        "outputId": "26f6f399-786a-43fb-c586-99dec3ec1463"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/UDL/install_pyspark.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsdkaEd97EHI",
        "outputId": "8739136f-acda-41fc-e9b2-870e3cf1cb22"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Install JAVA 8\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=84181c2033e5cb1144f1e4db485c7f78fef4af0004158908c05606a053b5b277\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Obtaining last version of spark\n",
            "/content/drive/MyDrive/UDL/install_pyspark.py:17: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 17 of the file /content/drive/MyDrive/UDL/install_pyspark.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  soup = BeautifulSoup(html_doc)\n",
            "Getting version spark-3.2.1\n",
            "Downloading https://downloads.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop2.7.tgz\n",
            "Installing PySpark\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 281.4 MB 39 kB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198 kB 49.8 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Setting environment variables for JAVA_HOME and SPARK_HOME\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start working with Spark\n",
        "Now we now and understand how Spark appeared in our lives and more or less how it works (and you know, it's amazing ü§≠), we can start to work with it.\n",
        "As you now, the SparkSession is the way programmers \"talk\" with Spark. So, we need to inicialize that."
      ],
      "metadata": {
        "id": "u3_oicKBMsEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (SparkSession\n",
        " .builder\n",
        " .appName(\"example\")\n",
        " .getOrCreate())"
      ],
      "metadata": {
        "id": "ekZc-YctYpwJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a DF to program the example mentioned in slides"
      ],
      "metadata": {
        "id": "uQHcAB5cOBxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame([(\"juan fernando\", 20), (\"valentina laverde\", 31), (\"teresa s√°nchez\", 30), (\"julieta ponce\", 35), (\"antonio garc√≠a\", 25)], [\"name\", \"age\"])\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYZQkqf37lm4",
        "outputId": "abbd195b-b46a-4ba6-e19d-b5deb5cf51d7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+---+\n",
            "|             name|age|\n",
            "+-----------------+---+\n",
            "|    juan fernando| 20|\n",
            "|valentina laverde| 31|\n",
            "|   teresa s√°nchez| 30|\n",
            "|    julieta ponce| 35|\n",
            "|   antonio garc√≠a| 25|\n",
            "+-----------------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember, what we want is to convert the first letter to capital case.\n",
        "Fist, we need to create a python function, that from a given input (string) it converts the value into same string with first letter as capital case letter."
      ],
      "metadata": {
        "id": "fU_y-oj-c3HS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convertCase(lower_string):\n",
        "    result=\"\"\n",
        "    arr = lower_string.split(\" \")\n",
        "    for x in arr:\n",
        "       result= result + x[0].upper() + x[1:len(x)] + \" \"\n",
        "    return result[0:-1] "
      ],
      "metadata": {
        "id": "IoDgTh6Pc2ON"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we convert the funciton to udf (the default type of UDF is StringType)"
      ],
      "metadata": {
        "id": "DyPo6_kZeAvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "convertUDF = F.udf(lambda z: convertCase(z)) "
      ],
      "metadata": {
        "id": "UXKli4qed7Z4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can use the convertUDF, as a function of sparkSQL, for example, in a select() or in a withColumn() call"
      ],
      "metadata": {
        "id": "DjmmQwzIedgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(convertUDF(F.col(\"name\")).alias(\"name\"), F.col(\"age\") ) \\\n",
        "   .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-HzXmlfeZUe",
        "outputId": "47ba4741-ebcf-419e-ffd9-88e020fcb4d0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+---+\n",
            "|name             |age|\n",
            "+-----------------+---+\n",
            "|Juan Fernando    |20 |\n",
            "|Valentina Laverde|31 |\n",
            "|Teresa S√°nchez   |30 |\n",
            "|Julieta Ponce    |35 |\n",
            "|Antonio Garc√≠a   |25 |\n",
            "+-----------------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn(\"corrected name\", convertUDF(F.col(\"name\")))\\\n",
        "  .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMu8m3zme5-9",
        "outputId": "cde939eb-e730-498e-de04-8ff45aae7e8c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+---+-----------------+\n",
            "|name             |age|corrected name   |\n",
            "+-----------------+---+-----------------+\n",
            "|juan fernando    |20 |Juan Fernando    |\n",
            "|valentina laverde|31 |Valentina Laverde|\n",
            "|teresa s√°nchez   |30 |Teresa S√°nchez   |\n",
            "|julieta ponce    |35 |Julieta Ponce    |\n",
            "|antonio garc√≠a   |25 |Antonio Garc√≠a   |\n",
            "+-----------------+---+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also use our UDF on SQL"
      ],
      "metadata": {
        "id": "0beyu5DqfmIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.types as T\n",
        "spark.udf.register(\"convertUDF\", convertCase, T.StringType())\n",
        "df.createOrReplaceTempView(\"NAMES\")\n",
        "spark.sql(\"select convertUDF(name) as name, age from NAMES\") \\\n",
        "     .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piR_nNhxfH3U",
        "outputId": "570b8da3-990a-4a71-9b2b-c29238d2d434"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+---+\n",
            "|name             |age|\n",
            "+-----------------+---+\n",
            "|Juan Fernando    |20 |\n",
            "|Valentina Laverde|31 |\n",
            "|Teresa S√°nchez   |30 |\n",
            "|Julieta Ponce    |35 |\n",
            "|Antonio Garc√≠a   |25 |\n",
            "+-----------------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way to create UDF method, is to use the annotation @udf(resturnType=\\<type\\>) above the method definition"
      ],
      "metadata": {
        "id": "BkD0YH-vg5nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@F.udf(returnType=T.StringType()) \n",
        "def upperCase(str):\n",
        "    return str.upper()\n"
      ],
      "metadata": {
        "id": "gjRGgRBSgHGs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn(\"Upper Name\", upperCase(F.col(\"Name\"))) \\\n",
        ".show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LN_z9kkhQd8",
        "outputId": "05cd4cc9-5624-44bd-d716-28e8fc634931"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+---+-----------------+\n",
            "|name             |age|Upper Name       |\n",
            "+-----------------+---+-----------------+\n",
            "|juan fernando    |20 |JUAN FERNANDO    |\n",
            "|valentina laverde|31 |VALENTINA LAVERDE|\n",
            "|teresa s√°nchez   |30 |TERESA S√ÅNCHEZ   |\n",
            "|julieta ponce    |35 |JULIETA PONCE    |\n",
            "|antonio garc√≠a   |25 |ANTONIO GARC√çA   |\n",
            "+-----------------+---+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3WHG3XNazyEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling null check"
      ],
      "metadata": {
        "id": "5WLB9TyljwAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_nulls = spark.createDataFrame([(\"juan fernando\", 20), (None, 31), (\"teresa s√°nchez\", 30), (\"julieta ponce\", 35), (\"antonio garc√≠a\", 25)], [\"name\", \"age\"])\n",
        "df_nulls.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwQn_-6gj0Fg",
        "outputId": "453ccf43-43e0-4556-c62b-1235bad30f19"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+---+\n",
            "|          name|age|\n",
            "+--------------+---+\n",
            "| juan fernando| 20|\n",
            "|          null| 31|\n",
            "|teresa s√°nchez| 30|\n",
            "| julieta ponce| 35|\n",
            "|antonio garc√≠a| 25|\n",
            "+--------------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_nulls.createOrReplaceTempView(\"NAMES_NULLS\")\n",
        "spark.sql(\"select convertUDF(name) as Name from NAMES_NULLS \" + \\\n",
        "         \"where name is not null and convertUDF(name) like '%Juan%'\") \\\n",
        "     .show(truncate=False) \n",
        "#IT COULD FAIL if the udf is executed befoure the not null check"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHzNcCUyhXg2",
        "outputId": "c8d0aa50-7bd4-4a88-cf9e-80e0df3e2060"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "|Name         |\n",
            "+-------------+\n",
            "|Juan Fernando|\n",
            "+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To aboid this, we can filter nulls in the registration of the UDF"
      ],
      "metadata": {
        "id": "ngLDG9sj1KGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.udf.register(\"_nullsafeUDF\", lambda str: convertCase(str) if not str is None else \"\" , T.StringType())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zO-Js0wifnW",
        "outputId": "693084a4-a1c3-492e-b0a0-1221d86bee38"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.<lambda>>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select _nullsafeUDF(name) as Name from NAMES_NULLS \" + \\\n",
        "         \"where _nullsafeUDF(name) like '%Juan%'\") \\\n",
        "     .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvVe8Adnjjmq",
        "outputId": "82e8977b-4f45-47a3-a2ea-a38252867eb9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "|Name         |\n",
            "+-------------+\n",
            "|Juan Fernando|\n",
            "+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1:\n",
        "\n",
        "\n",
        "\n",
        "*   Get data from the CSV: https://raw.githubusercontent.com/tidyverse/ggplot2/main/data-raw/diamonds.csv and save it in a dataframe.\n",
        "*   Generate a new column, called \"cut_color_id\". This column, will have the first letter of the *cut* column, and the *color* column value. As an example, if the *cut* is \"Premium\" and the *color* is \"I\", the result in the new column will be \"PI\". Do it with a UDF.\n",
        "*   Take into account, is better to use the functions of spark, if we can, because they are more optized than UDFs. Do you know how to do the same without an UDF? Do it.\n",
        "\n"
      ],
      "metadata": {
        "id": "046hSw2A28NG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Caching and Persistence of Data\n"
      ],
      "metadata": {
        "id": "k5Q2_HiZTJs1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataFrame.cache()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H2R7JYa2TOZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_to_cache = spark.range(1*10000000).toDF(\"id\").withColumn(\"sqaure\", F.col(\"id\")*F.col(\"id\"))\n",
        "df_to_cache.show()"
      ],
      "metadata": {
        "id": "wXANSZ-ijl6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68649ac9-33ee-423a-ecfd-a711eeb256b2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "| id|sqaure|\n",
            "+---+------+\n",
            "|  0|     0|\n",
            "|  1|     1|\n",
            "|  2|     4|\n",
            "|  3|     9|\n",
            "|  4|    16|\n",
            "|  5|    25|\n",
            "|  6|    36|\n",
            "|  7|    49|\n",
            "|  8|    64|\n",
            "|  9|    81|\n",
            "| 10|   100|\n",
            "| 11|   121|\n",
            "| 12|   144|\n",
            "| 13|   169|\n",
            "| 14|   196|\n",
            "| 15|   225|\n",
            "| 16|   256|\n",
            "| 17|   289|\n",
            "| 18|   324|\n",
            "| 19|   361|\n",
            "+---+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cache this data\n",
        "df_to_cache.cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxFpdRGGTUR0",
        "outputId": "77e1c2ca-289d-48a4-bd7e-0f47b3a0427f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, sqaure: bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "startTimeQuery = time.process_time()\n",
        "df_to_cache.count()\n",
        "endTimeQuery = time.process_time()\n",
        "endTimeQuery - startTimeQuery"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uursOPSTs-U",
        "outputId": "bc1c1eb3-6e23-41a8-b264-345a42ff4823"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.05209608299999857"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "startTimeQuery = time.process_time()\n",
        "df_to_cache.count()\n",
        "endTimeQuery = time.process_time()\n",
        "endTimeQuery - startTimeQuery"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTJFJCKCTwZE",
        "outputId": "6d9a170d-d700-4174-bcac-bf2362eeb173"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0006143160000000591"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_to_persist = spark.range(10001000).toDF(\"id2\").withColumn(\"sqaure\", F.col(\"id2\")*F.col(\"id2\"))"
      ],
      "metadata": {
        "id": "9fsr6DhvVdfN"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.storagelevel import StorageLevel\n",
        "#persist this data\n",
        "df_to_persist.persist(StorageLevel.DISK_ONLY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF0NT9kQYsVn",
        "outputId": "baa98bec-5b18-4069-a54a-9e614119836e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id2: bigint, sqaure: bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "startTimeQuery = time.process_time()\n",
        "df_to_persist.count()\n",
        "endTimeQuery = time.process_time()\n",
        "endTimeQuery - startTimeQuery"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVWjyivuY5bO",
        "outputId": "a8b2e7d2-cdc7-4e00-c199-b1bbf64a4330"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.047231854000001405"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "startTimeQuery = time.process_time()\n",
        "df_to_persist.count()\n",
        "endTimeQuery = time.process_time()\n",
        "endTimeQuery - startTimeQuery"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNWrI4mVY_N2",
        "outputId": "0944526e-ff3c-44d9-91bf-edb7b3fb282c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0037607709999996075"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As this data is now saved on disk, after use it, we are going to erase it."
      ],
      "metadata": {
        "id": "yKZCvvN0aptG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_to_persist.unpersist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQvN6--oZpde",
        "outputId": "7da22dac-cb4f-42f6-982e-d7e5c82a78f0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id2: bigint, sqaure: bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_to_cache.unpersist()"
      ],
      "metadata": {
        "id": "E65s2RFdazf-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2babe2fc-4389-4033-d9f0-c5a5b46bdca6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, sqaure: bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3O0toVoM0OQu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}