{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIDPhiigMKQI"
      },
      "source": [
        "# Apache Spark's Structured APIs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgS3nhjaMQQt"
      },
      "source": [
        "## Prepare environment\n",
        "First, we are going to prepare the environment for running PySaprk in the Google Collab Machine (if you work directly in your computer, and you want to prepare it, read and follow champter 2 instructions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTEPUoaL69gw"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsdkaEd97EHI"
      },
      "outputs": [],
      "source": [
        "!python /content/drive/MyDrive/UDL/2024/install_pyspark.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3_oicKBMsEO"
      },
      "source": [
        "## Start working with Spark\n",
        "Now we now and understand how Spark appeared in our lives and more or less how it works (and you know, it's amazing ðŸ¤­), we can start to work with it.\n",
        "As you now, the SparkSession is the way programmers \"talk\" with Spark. So, we need to inicialize that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekZc-YctYpwJ"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (SparkSession\n",
        " .builder\n",
        " .appName(\"example\")\n",
        " .getOrCreate())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqFOiyErNuHk"
      },
      "source": [
        "## Example of working with RDDs\n",
        "But remember, since Spark 2.X we have Structured Data APIs and WE ðŸ§¡ DF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "es7-0PCeXvHx"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext\n",
        "# Create an RDD of tuples (name, age)\n",
        "dataRDD = spark.sparkContext.parallelize([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30), (\"TD\", 35), (\"Brooke\", 25)])\n",
        "# Use map and reduceByKey transformations with their lambda\n",
        "# expressions to aggregate and then compute average\n",
        "agesRDD = (dataRDD\n",
        ".map(lambda x: (x[0], (x[1], 1)))\n",
        ".reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
        ".map(lambda x: (x[0], x[1][0]/x[1][1])))\n",
        "agesRDD.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQHcAB5cOBxV"
      },
      "source": [
        "## Example of working with DFs\n",
        "Yep, as you can see it's easier, clearlier, more bueatiful... (it looks like pandas, isn't it?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYZQkqf37lm4"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import avg\n",
        "\n",
        "data_df = spark.createDataFrame([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30), (\"TD\", 35), (\"Brooke\", 25)], [\"name\", \"age\"])\n",
        "# Group the same names together, aggregate their ages, and compute an average\n",
        "avg_df = data_df.groupBy(\"name\").agg(avg(\"age\"))\n",
        "# Show the results of the final execution\n",
        "avg_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPvhaoAJO08c"
      },
      "source": [
        "## Define schemas\n",
        "There are two ways to defines them:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4BznmGnPAPU"
      },
      "source": [
        "### Define the schema programatically"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGt75kO4Y1Kf"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import *\n",
        "schema_programatically = StructType([StructField(\"Id\", IntegerType(), False),\n",
        "  StructField(\"First\", StringType(), False),\n",
        "  StructField(\"Last\", StringType(), False),\n",
        "  StructField(\"Url\", StringType(), False),\n",
        "  StructField(\"Published\", StringType(), False),\n",
        "  StructField(\"Hints\", IntegerType(), False),\n",
        "  StructField(\"Campaigns\", ArrayType(StringType()), False),\n",
        " ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93lpj2Y1PZec"
      },
      "source": [
        "### Define the schema using DDL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qri60GusPUMb"
      },
      "outputs": [],
      "source": [
        "schema_ddl = \"`Id` INT, `First` STRING, `Last` STRING, `Url` STRING, `Published` STRING, `Hits` INT, `Campaigns` ARRAY<STRING>\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rraMBoX2Qcxs"
      },
      "source": [
        "### Example creating data with both"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-8imTWOPg06"
      },
      "outputs": [],
      "source": [
        "#create our data\n",
        "data = [[1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\", \"LinkedIn\"]],\n",
        "       [2, \"Brooke\",\"Wenig\",\"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\", \"LinkedIn\"]],\n",
        "       [3, \"Denny\", \"Lee\", \"https://tinyurl.3\",\"6/7/2019\",7659, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
        "       [4, \"Tathagata\", \"Das\",\"https://tinyurl.4\", \"5/12/2018\", 10568, [\"twitter\", \"FB\"]],\n",
        "       [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
        "       [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568, [\"twitter\", \"LinkedIn\"]]\n",
        "      ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXK1ZHkIQnMc"
      },
      "outputs": [],
      "source": [
        "# create a DataFrame using the schema built programatically\n",
        "blogs_df = spark.createDataFrame(data, schema_programatically)\n",
        "blogs_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Do7xvaglQt-7"
      },
      "outputs": [],
      "source": [
        "# create a DataFrame using the schema built programatically\n",
        "blogs_df = spark.createDataFrame(data, schema_ddl)\n",
        "blogs_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4eEQanYRtT1"
      },
      "source": [
        "## Exercice 1\n",
        "As you can see, the previous schema are not exactly the same. In the programatically way, we can specify not nulleable values. Any idea about how to do the same with DDL?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvMQqV0gQ4WL"
      },
      "outputs": [],
      "source": [
        "#pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaXI16gfV9UV"
      },
      "source": [
        "### Get Schema from DF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W--2O1Q5SKZZ"
      },
      "outputs": [],
      "source": [
        "blogs_df.schema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z095l-qbWL2F"
      },
      "source": [
        "### Read data from json\n",
        "Download data from https://github.com/databricks/LearningSparkV2/blob/master/chapter3/scala/data/blogs.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aw7yuLR2WEwj"
      },
      "outputs": [],
      "source": [
        "import wget\n",
        "json_url = \"https://raw.githubusercontent.com/databricks/LearningSparkV2/master/chapter3/scala/data/blogs.json\"\n",
        "wget.download(json_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuoB82W1Wiir"
      },
      "outputs": [],
      "source": [
        "blogs_df = spark.read.json('blogs.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s11KbcoFWwKs"
      },
      "outputs": [],
      "source": [
        "blogs_df = spark.read.schema(schema_ddl).json('blogs.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJejny9IcU19"
      },
      "source": [
        "*Return to slides*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A05ZxsG1caP2"
      },
      "source": [
        "## Columns and Expressions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ij1IHStchql"
      },
      "source": [
        "### List all columns of DF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUEAaIuFcYU6"
      },
      "outputs": [],
      "source": [
        "blogs_df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQPlsmT8cq1m"
      },
      "source": [
        "### Access to particular columns with col function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AlQt8vlcn_M"
      },
      "outputs": [],
      "source": [
        "blogs_df[\"Id\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mni6yWQDdkt1"
      },
      "source": [
        "### Different ways of computing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnhzQKxed0XF"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import expr\n",
        "blogs_df.select(expr(\"Hits * 2\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdNWf99_cz1s"
      },
      "outputs": [],
      "source": [
        "blogs_df.selectExpr(\"Hits * 2\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmiYRZNudtTB"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "blogs_df.select(col(\"Hits\") * 2).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfIqnLEOgmj1"
      },
      "source": [
        "### Create new columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8gBzvQReLGD"
      },
      "outputs": [],
      "source": [
        "blogs_df.withColumn(\"Big Hitters\", col(\"Hits\")>10000).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjXMt1mNgDQ9"
      },
      "outputs": [],
      "source": [
        "# big_hitters_df = blogs_df.withColumn(\"Big Hitters\", col(\"Hits\")>10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtBUy_yfqOTd"
      },
      "source": [
        "For our mental health: import pyspark.sql.functions as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-zF08ftgYt5"
      },
      "outputs": [],
      "source": [
        "import pyspark.sql.functions as F\n",
        "blogs_df \\\n",
        "  .withColumn(\"CompleteName\", F.concat(F.col(\"First\"), F.lit(\"\"), F.col(\"Last\"))).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHXYEPB7rFzF"
      },
      "source": [
        "### Select (project) some columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFxSQNSkrD7V"
      },
      "outputs": [],
      "source": [
        "blogs_df.select(F.col(\"Hits\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajvmvRCJhGgi"
      },
      "outputs": [],
      "source": [
        "blogs_df.select(\"Hits\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIsXZ5u2rbPc"
      },
      "outputs": [],
      "source": [
        "blogs_df.select(F.col(\"Hits\"), F.col(\"Id\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfXIQ3TlreV8"
      },
      "outputs": [],
      "source": [
        "blogs_df.select([\"Hits\", \"Id\"]).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc6KEDMmr3I_"
      },
      "source": [
        "### Sort values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Bhbz6KdrjxT"
      },
      "outputs": [],
      "source": [
        "blogs_df.sort(col(\"Hits\"),ascending=False).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3Ty_R43sgRN"
      },
      "source": [
        "*Return to slides*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGoLAIW4tCdd"
      },
      "source": [
        "## Rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQqIXwqVtYYN"
      },
      "source": [
        "We can get all the df's records as a list of :class:`Row`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lwdkpdi1sNt0"
      },
      "outputs": [],
      "source": [
        "blogs_df.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U08HHoWmtlDd"
      },
      "source": [
        "We also can create Row objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jV1ep-mKtLBE"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import Row\n",
        "blog_row = Row(6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", 255568, \"3/2/2015\", [\"twitter\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDbghyNtuDGs"
      },
      "source": [
        "We can access elements in row by position:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwSK4bmdt3oi"
      },
      "outputs": [],
      "source": [
        "blog_row[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKMPtyBluJvc"
      },
      "source": [
        "or by column name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umsNIb_WuHo8"
      },
      "outputs": [],
      "source": [
        "blog_row[\"Id\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3cNGluvul91"
      },
      "source": [
        "The problem here, is that Spark doen't know the columns names... to fix we could create it like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpqfHHZhuM4z"
      },
      "outputs": [],
      "source": [
        "blog_named_row = Row(Id=6, First=\"Reynold\", Last=\"Xin\", Url=\"https://tinyurl.6\", Hits=255568, Published=\"3/2/2015\", Campaigns=[\"twitter\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8SR5n_CvaL0"
      },
      "outputs": [],
      "source": [
        "blog_named_row[\"Id\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAyGkrjLvlId"
      },
      "source": [
        "We can create a DF from a list of  Raws"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7agENUwnvcK7"
      },
      "outputs": [],
      "source": [
        "df_from_named_raws = spark.createDataFrame([blog_named_row])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBmXoYACvr6E"
      },
      "outputs": [],
      "source": [
        "df_from_raws = spark.createDataFrame([blog_row], ['Id', 'First', 'Last', 'Url', 'Hits', 'Published', 'Campaigns'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOxO9POyxEct"
      },
      "source": [
        "## Write results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LnOEGW1wgq1"
      },
      "outputs": [],
      "source": [
        "parquet_path= \"blogs_df.parquet\"\n",
        "blogs_df.write.parquet(parquet_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uQtxV_txOaJ"
      },
      "outputs": [],
      "source": [
        "json_path= \"blogs_df.json\"\n",
        "blogs_df.write.json(json_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFLhLRybwfxF"
      },
      "source": [
        "Return to slides"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz_l_4_N0ACG"
      },
      "source": [
        "## Filters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFL0wqJB0AuE"
      },
      "outputs": [],
      "source": [
        "blogs_df.filter(F.col(\"Hits\") > 10000).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RumSUJ4K0NQs"
      },
      "outputs": [],
      "source": [
        "blogs_df.where(F.col(\"Hits\") > 10000).where(F.array_contains(F.col(\"Campaigns\"),\"FB\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rc8ZMEFI0R97"
      },
      "outputs": [],
      "source": [
        "wget.download('https://github.com/databricks/LearningSparkV2/raw/master/chapter3/data/sf-fire-calls.csv')\n",
        "fire_df = spark.read.csv('sf-fire-calls.csv', header = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCMK1Mtz4_pj"
      },
      "outputs": [],
      "source": [
        "fire_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amyk979n5c8c"
      },
      "outputs": [],
      "source": [
        "medical_inc_df = fire_df\\\n",
        "  .select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\")\\\n",
        "  .where(F.col(\"callType\") != \"Medical Incident\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7LKQ35W5-O8"
      },
      "outputs": [],
      "source": [
        "medical_inc_df.show(5, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAbR224s6PSW"
      },
      "source": [
        "### Aggregations\n",
        "Let's imagine, we want to count different kinds of calls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AuYnQtu6C-0"
      },
      "outputs": [],
      "source": [
        "fire_df\\\n",
        "  .select(\"CallType\")\\\n",
        "  .where(F.col(\"CallType\").isNotNull())\\\n",
        "  .agg(F.count_distinct(\"CallType\").alias(\"diff_types\"))\\\n",
        "  .show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1UvXZHN6-JN"
      },
      "source": [
        "And list them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qckjRHnr6oPV"
      },
      "outputs": [],
      "source": [
        "fire_df\\\n",
        "  .select(\"CallType\")\\\n",
        "  .where(F.col(\"CallType\").isNotNull())\\\n",
        "  .distinct()\\\n",
        "  .show(30, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMOCGmCX7by2"
      },
      "source": [
        " ### Renaming, adding and dropping columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6p9Afde7IQU"
      },
      "outputs": [],
      "source": [
        "new_fire_df = fire_df.withColumnRenamed(\"Delay\", \"ResponseDelayedinMins\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDNCy13KAycj"
      },
      "outputs": [],
      "source": [
        "new_fire_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Tj_YYyuGC0I"
      },
      "source": [
        "Now, usual data processing: change dates to timestamps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4W8RTGT0A5st"
      },
      "outputs": [],
      "source": [
        "fire_ts_df = (new_fire_df\n",
        " .withColumn(\"IncidentDate\", F.to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\"))\n",
        " .drop(\"CallDate\")\n",
        " .withColumn(\"OnWatchDate\", F.to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\"))\n",
        " .drop(\"WatchDate\")\n",
        " .withColumn(\"AvailableDtTS\", F.to_timestamp(col(\"AvailableDtTm\"),\n",
        " \"MM/dd/yyyy hh:mm:ss a\"))\n",
        " .drop(\"AvailableDtTm\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIAI5CVXGVy9"
      },
      "outputs": [],
      "source": [
        "fire_ts_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHyKtVCnJqbJ"
      },
      "source": [
        "### Aggregations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mNH8N7kJ_gm"
      },
      "source": [
        " + What are the most common types of fire calls?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkF5xskDGbrl"
      },
      "outputs": [],
      "source": [
        "(fire_ts_df\n",
        " .select(\"CallType\")\n",
        " .where(col(\"CallType\").isNotNull())\n",
        " .groupBy(\"CallType\")\n",
        " .count() # it's a kind of aggregation\n",
        " .orderBy(\"count\", ascending=False)\n",
        " .show(n=10, truncate=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6yOG_t8J9Ve"
      },
      "outputs": [],
      "source": [
        "(fire_ts_df\n",
        " .select(F.sum(\"NumAlarms\"),\n",
        "         F.avg(\"ResponseDelayedinMins\"),\n",
        "         F.min(\"ResponseDelayedinMins\"),\n",
        "         F.max(\"ResponseDelayedinMins\"))\n",
        " .show())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEDGCFxbN87f"
      },
      "source": [
        "+ What were all the different types of fire calls in 2018?\n",
        "+ What months within the year 2018 saw the highest number of fire calls?\n",
        "+ Which neighborhood in San Francisco generated the most fire calls in 2018?\n",
        "+ Which neighborhoods had the worst response times to fire calls in 2018?\n",
        "+ Which week in the year in 2018 had the most fire calls?\n",
        "+ Is there a correlation between neighborhood, zip code, and number of fire calls?\n",
        "+ How can we use Parquet files or SQL tables to store this data and read it back?\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
